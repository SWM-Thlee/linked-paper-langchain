# Stage 1: 모델을 미리 다운로드하여 캐시하는 빌더 이미지
FROM python:3.9-slim AS model-builder

RUN pip install -U "huggingface_hub[cli]"

# BAAI/bge-reranker-v2-m3 모델의 필요한 파일 다운로드 및 캐시 저장
RUN huggingface-cli download BAAI/bge-reranker-v2-m3 config.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 sentencepiece.bpe.model --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 tokenizer.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 model.safetensors --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 special_tokens_map.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 tokenizer_config.json --cache-dir=/model_cache

# BAAI/bge-m3 모델의 전체 파일 다운로드 및 캐시 저장
RUN huggingface-cli download BAAI/bge-m3 config_sentence_transformers.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 modules.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 README.md --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 sentence_bert_config.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 config.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 pytorch_model.bin --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 tokenizer_config.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 sentencepiece.bpe.model --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 tokenizer.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 special_tokens_map.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 1_Pooling/config.json --cache-dir=/model_cache


# Stage 2: 최종 런타임 이미지
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY ./src ./src

WORKDIR /app/src

# Stage 1에서 모델 캐시 파일을 복사
COPY --from=model-builder /model_cache /root/.cache/huggingface/hub

EXPOSE 8000

ENV ENVIRONMENT=prod
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
ENV CUDA_VISIBLE_DEVICES=0

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
